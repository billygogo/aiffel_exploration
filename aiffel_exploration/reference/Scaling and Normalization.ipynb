{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0238f295",
   "metadata": {},
   "source": [
    "# Q1) Why do you have to change characters into real numbers in a machine learning model? Does that help?\n",
    "왜 숫자로 변환해야만 하는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de2a3ee",
   "metadata": {},
   "source": [
    "Machine learning algorithms are designed to work with numerical data, as they require mathematical operations to learn from the input data and make predictions. This means that any input data that is not already in numerical form must be transformed into a numerical representation before it can be used by the algorithm.\n",
    "\n",
    "There are several reasons why converting non-numerical data into numerical data can be beneficial for a machine learning model:\n",
    "\n",
    "* Mathematical operations: Machine learning algorithms use mathematical operations to process and analyze data. Numerical data can be easily manipulated and combined using these operations.\n",
    "\n",
    "* Simplify data representation: Converting non-numerical data into numerical data can simplify the data representation, making it easier for the algorithm to learn patterns and relationships within the data.\n",
    "\n",
    "* Enable the use of standard algorithms: Many machine learning algorithms are designed to work with numerical data, so converting non-numerical data into numerical data allows you to use a wider range of standard algorithms.\n",
    "\n",
    "There are several ways to transform non-numerical data into numerical data, such as one-hot encoding, label encoding, and feature scaling. The choice of transformation method depends on the type and nature of the data.\n",
    "\n",
    "In summary, converting non-numerical data into numerical data is essential for using machine learning algorithms, as it enables mathematical operations and simplifies the data representation, making it easier for the algorithm to learn from the data and make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db25e24",
   "metadata": {},
   "source": [
    "# Q2) Why do log scaling?\n",
    "로그 스케일링을 왜 하나?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ac57e3",
   "metadata": {},
   "source": [
    "Log scaling is a technique used in data preprocessing to transform numeric features by taking the logarithm of the original values. This technique is used to address issues with data that has a large range of values or contains outliers.\n",
    "\n",
    "One reason to use log scaling is to normalize the distribution of the data. For example, some features may have a skewed distribution where most of the values are concentrated at one end of the range. This can lead to issues with certain machine learning algorithms, such as linear regression or neural networks, that assume a normal distribution of data. By taking the logarithm of the values, the data can be transformed into a more normal distribution.\n",
    "\n",
    "Another reason to use log scaling is to address the impact of outliers. Outliers are data points that are significantly different from the rest of the data and can skew the model. By taking the logarithm of the values, the impact of outliers is reduced, as the logarithmic transformation reduces the distance between the extreme values.\n",
    "\n",
    "In addition, log scaling can help to scale down large numbers, which can be useful in scenarios where the range of values is very large. For example, in finance, stock prices can range from a few cents to thousands of dollars, which can make it difficult to analyze the data without scaling. Log scaling can help to compress this range of values and make the data easier to work with.\n",
    "\n",
    "In summary, log scaling is a useful technique in data preprocessing that can help to normalize the distribution of the data, address the impact of outliers, and scale down large numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38f88d0",
   "metadata": {},
   "source": [
    "# Q3) some numbers affect machine performance?\n",
    "숫자를 사용하는 것이 머신러닝 성능에 영향을 주는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0c017",
   "metadata": {},
   "source": [
    "Yes, the numbers used as input data for a machine learning model can significantly affect its performance. Here are some examples:\n",
    "\n",
    "* Scaling of numeric features: The range of numeric features can impact the performance of a model, as some algorithms are sensitive to the scaling of features. For example, if one feature has a large range of values while another feature has a small range of values, the algorithm may over-emphasize the importance of the larger feature. To mitigate this, feature scaling techniques such as normalization or standardization can be applied.\n",
    "\n",
    "* Outliers: Outliers in numeric data can skew the model and lead to incorrect predictions. Outliers may occur due to data entry errors or natural variability in the data. There are various techniques to detect and handle outliers, such as removing them or transforming the data.\n",
    "\n",
    "* Correlation between numeric features: Highly correlated features can cause redundancy in the model and negatively impact its performance. Feature selection or dimensionality reduction techniques can be used to address this issue.\n",
    "\n",
    "* Missing values: Missing values in numeric data can impact the performance of the model. Techniques such as imputation or removal of missing data can be used to address this issue.\n",
    "\n",
    "In summary, numeric data can significantly impact the performance of a machine learning model, and it is important to carefully preprocess and clean the data to mitigate potential issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7907bcd",
   "metadata": {},
   "source": [
    "# Q4) what's difference between Normalization and Log Scaling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534bcea6",
   "metadata": {},
   "source": [
    "Normalization and log scaling are two different methods of transforming numerical data in machine learning.\n",
    "\n",
    "Normalization, as explained in my previous answer, is a process of scaling the values of a numerical feature to a standard range, typically between 0 and 1 or -1 and 1. Normalization is used to ensure that all features are on the same scale and to avoid bias towards features with larger values.\n",
    "\n",
    "Log scaling, on the other hand, is a transformation that involves taking the logarithm of each value in a numerical feature. The most common type of log scaling is natural logarithm, denoted as ln(x). The purpose of log scaling is to reduce the range of the data and make it easier to visualize patterns in the data.\n",
    "\n",
    "Log scaling is particularly useful when the data is skewed or has a wide range of values. By taking the logarithm of the values, the distribution of the data is compressed and the extreme values are brought closer to the mean. This can make it easier to see patterns in the data and can help improve the performance of some machine learning algorithms.\n",
    "\n",
    "Overall, normalization and log scaling are both methods for transforming numerical data in machine learning, but they serve different purposes. Normalization is used to ensure that all features are on the same scale and to avoid bias towards features with larger values, while log scaling is used to reduce the range of the data and make it easier to visualize patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b64c13",
   "metadata": {},
   "source": [
    "## Q4-1) code sample normalization in ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82826a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create an instance of the MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e228ce7",
   "metadata": {},
   "source": [
    "In this example, we are using the MinMaxScaler class from scikit-learn to perform normalization. First, we create an instance of the scaler object. Then, we fit the scaler to the training data using the fit_transform method, which computes the minimum and maximum values of each feature in the training data and scales the values to be between 0 and 1. Finally, we use the transform method to apply the same transformation to the test data.\n",
    "\n",
    "Note that you should always fit the scaler to the training data only, and then use the same scaler to transform both the training and test data. This ensures that the scaling is consistent across both sets of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79e0f75",
   "metadata": {},
   "source": [
    "## Q4-2)show me data sample normalization in ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29faa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "Original Data:\n",
    "    Feature 1: [1, 2, 3, 4, 5]\n",
    "    Feature 2: [10, 20, 30, 40, 50]\n",
    "\n",
    "Normalized Data:\n",
    "    Feature 1: [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "    Feature 2: [0.0, 0.25, 0.5, 0.75, 1.0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d4abf3",
   "metadata": {},
   "source": [
    "In this example, we have a dataset with two features: Feature 1 and Feature 2. Feature 1 has values ranging from 1 to 5, while Feature 2 has values ranging from 10 to 50.\n",
    "\n",
    "To normalize the data, we use the Min-Max scaling method, which scales each feature to a range between 0 and 1. We first compute the minimum and maximum values of each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77759a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "    Feature 1: min=1, max=5\n",
    "    Feature 2: min=10, max=50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9938f59",
   "metadata": {},
   "source": [
    "Then, we use the following formula to scale each value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf27f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    scaled_value = (value - min_value) / (max_value - min_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d5471f",
   "metadata": {},
   "source": [
    "Applying this formula to each value in the original data yields the normalized data shown above.\n",
    "\n",
    "Note that the same scaling method can be applied to much larger datasets with many more features, and that normalization is often a necessary preprocessing step for many machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5fe913",
   "metadata": {},
   "source": [
    "# Q5)show me code sample about scaling in ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be26597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create an instance of the StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2025b39",
   "metadata": {},
   "source": [
    "In this example, we are using the StandardScaler class from scikit-learn to perform scaling. First, we create an instance of the scaler object. Then, we fit the scaler to the training data using the fit_transform method, which computes the mean and standard deviation of each feature in the training data and scales the values to have zero mean and unit variance. Finally, we use the transform method to apply the same transformation to the test data.\n",
    "\n",
    "Note that you should always fit the scaler to the training data only, and then use the same scaler to transform both the training and test data. This ensures that the scaling is consistent across both sets of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477a1b34",
   "metadata": {},
   "source": [
    "\n",
    "# Q5-1) show me data sample about scaling in ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "Original Data:\n",
    "    Feature 1: [1, 2, 3, 4, 5]\n",
    "    Feature 2: [10, 20, 30, 40, 50]\n",
    "\n",
    "Scaled Data:\n",
    "    Feature 1: [-1.264, -0.632, 0.0, 0.632, 1.264]\n",
    "    Feature 2: [-1.264, -0.632, 0.0, 0.632, 1.264]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c74f5",
   "metadata": {},
   "source": [
    "In this example, we are using the standard scaling method to scale the data. The standard scaling method, also known as Z-score scaling, scales each feature to have zero mean and unit variance. To perform this scaling, we first compute the mean and standard deviation of each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b181223",
   "metadata": {},
   "outputs": [],
   "source": [
    "    Feature 1: mean=3, std=1.581\n",
    "    Feature 2: mean=30, std=15.811\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd4515",
   "metadata": {},
   "source": [
    "In this example, we are using the standard scaling method to scale the data. The standard scaling method, also known as Z-score scaling, scales each feature to have zero mean and unit variance. To perform this scaling, we first compute the mean and standard deviation of each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d8c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "    Feature 1: mean=3, std=1.581\n",
    "    Feature 2: mean=30, std=15.811\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3364c4d",
   "metadata": {},
   "source": [
    "We then use the following formula to scale each value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201fff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    scaled_value = (value - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c6afd1",
   "metadata": {},
   "source": [
    "Applying this formula to each value in the original data yields the scaled data shown above.\n",
    "\n",
    "Note that scaling can help to improve the performance of some machine learning algorithms, especially those that are sensitive to the scale of the input features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
